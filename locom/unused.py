#elif (args.command == "group"): return group_main(args)
#elif (args.command == "filter"): return filter_main(args)
#elif (args.command == "diff"): return diff_main(args)
#elif (args.command == "scheck"): return simple_check_main(args)
#elif (args.command == "bloom"): return bloom_main(args)
#elif (args.command == "bloom"): return bloom2_main(args)
#elif (args.command == "build"): return build_main(args)
#elif (args.command == "bcheck"): return build_check_main(args)
#elif (args.command == "bcompress"): return bcompress_main(args)
#elif (args.command == "bloom"): return bloom_main(args)
#elif (args.command == "blfmm"): return blfmm_main(args)
#elif (args.command == "multim"): return multiple_main(args)

'''Unused parsers
    parser_group = subparsers.add_parser("group", help="Insert k-mer frequencies into buckets based on minimizers")
    parser_group.add_argument("-i", help="counting table in input [stdin]", type=str)
    parser_group.add_argument("-o", help="dump file listing the set of frequencies associated to it for each mm", type=str, required=True)
    parser_group.add_argument("-m", help="minimizer size", type=int, required=True)
    parser_group.add_argument("-p", help="collapse policy: [min, avg, max, set, maj, id]", type=str, default="id")
    parser_group.add_argument("-d", help="maximum delta, [disabled]", type=int)
    parser_group.add_argument("-r", help="replacement label for uncollapsable lists [None]")
    
    parser_scheck = subparsers.add_parser("scheck", help="Compute error given a collapsed bucketing. Non-numerical labels do not collide.")
    parser_scheck.add_argument("-i", help="counting table in input [stdin]", type=str)
    parser_scheck.add_argument("-b", help="dump file of collapse command", type=str, required=True)

    parser_filter = subparsers.add_parser("filter", help="Filter table for k-mers with given bucket value.")
    parser_filter.add_argument("-i", help="counting table in input [stdin]", type=str)
    parser_filter.add_argument("-o", help="filtered counting table listing all k-mers [stdout]", type=str, required=True)
    parser_filter.add_argument("-b", help="dump file of collapse command", type=str, required=True)
    parser_filter.add_argument("-r", help="k-mers with this label will pass the filter [None]", type=str)
    parser_filter.add_argument("-s", help="invert logic of -r option", action="store_false")

    parser_difference = subparsers.add_parser("diff", help="Generate difference table between estimated freqency and true ones.")
    parser_difference.add_argument("-i", help="counting table in input [stdin]", type=str)
    parser_difference.add_argument("-o", help="counting table listing all k-mers with their difference between the true and bucketed counter [stdout]", type=str)
    parser_difference.add_argument("-b", help="dump file of collapse command", type=str, required=True)

    parser_collapse = subparsers.add_parser("collapse", help="Collapse sets to one value")
    parser_collapse.add_argument("-i", help="dump file of mgroup command", type=str, required=True)
    parser_collapse.add_argument("-o", help="modified dump file", type=str, required=True)
    parser_collapse.add_argument("-m", help="collapse mode: [min, avg, max, set, maj]", type=str, default="avg")
    parser_collapse.add_argument("-d", help="maximum delta", type=int)
    parser_collapse.add_argument("-r", help="replacement label for uncollapsable lists [None]")

    parser_dim = subparsers.add_parser("dim", help="Dimension the bloom filter given a histogram and error rate.")
    parser_dim.add_argument("-i", help="input histogram [stdin]", type=str)
    parser_dim.add_argument("-e", help="epsilon", type=float, required=True)

    parser_bloom = subparsers.add_parser("bloom", help="Estimate total size of Bloom Filter method")
    parser_bloom.add_argument("-i", help="counting table in input", type=str)
    parser_bloom.add_argument("-o", help="bloom filter", type=str, required=True)
    parser_bloom.add_argument("-m", help="minimizer length", type=int, required=True)
    parser_bloom.add_argument("-e", help="epsilon", type=float, required=True)

    parser_bloom2 = subparsers.add_parser("bloom", help="Build the Bloom filter method")
    parser_bloom2.add_argument("-i", help="counting table in input", type=str)
    parser_bloom2.add_argument("-o", help="output csv file", type=str)
    parser_bloom2.add_argument("-m", help="minimizer length", type=int, required=True)
    parser_bloom2.add_argument("-e", help="epsilon", type=float, required=True)
    parser_bloom2.add_argument("-s", "--seed", help="random seed [42]", type=int, default=42)

    parser_bcheck = subparsers.add_parser("bcheck", help="check output of build command")
    parser_bcheck.add_argument("-i", help="counting table in input", type=str, required=True)
    parser_bcheck.add_argument("-b", "--buckets", help="list of output files generated by the build command", type=str, required=True, nargs='+')

    parser_compress = subparsers.add_parser("bcompress", help="compress bucket tables using GV3 compressed static function")
    parser_compress.add_argument("-i", help="input folder/basename", type=str, required=True)
    parser_compress.add_argument("-o", help="output folder/basename", type=str, required=True)
    parser_compress.add_argument("-l", help="list of minimizer lengths to retrieve the tables to compress", type=int, required=True, nargs='+')

    parser_bloom = subparsers.add_parser("bloom", help="Compress a counting table using a bloom filter + CSF")
    parser_bloom.add_argument("-i", help="counting table in input", type=str, required=True)
    parser_bloom.add_argument("-o", help="csv file to append to. Format: [k-mer length, optimal bloom size, size of CSF, number of FP]", type=str, required=True)
    parser_bloom.add_argument("-e", help="epsilon", type=float, required=False)
    parser_bloom.add_argument("-s", "--seed", help="random seed [42]", type=int, default=42)

    parser_blfmm = subparsers.add_parser("blfmm", help="Compress a counting table by minimizer bucketing + bloom filter (for the deltas) + CSF")
    parser_blfmm.add_argument("-i", help="counting table in input", type=str, required=True)
    parser_blfmm.add_argument("-o", help="csv file to append to. Format: [k-mer length, minimizer size, size of the buckets (CSF compressed), optimal bloom size, size of CSF, number of FP]", type=str, required=True)
    parser_blfmm.add_argument("-m", help="minimizer size", type=int, required=True)
    parser_blfmm.add_argument("-e", help="epsilon", type=float, required=False)
    parser_blfmm.add_argument("-s", "--seed", help="random seed [42]", type=int, default=42)

    multim_help = (
        "Build a minimizer frequency table based on (m, ..., k)-mers:\n" +
        "\tk is the k-mer length in the input frequency table,\n" +
        "\tm is the starting minimizer length,\n" +
        "\t... is a list of minimizer lengths between m and k used for recursively resolve collisions."
    )
    parser_multim = subparsers.add_parser("multim", help=multim_help)
    parser_multim.add_argument("-i", help="counting table in input", type=str, required=True)
    parser_multim.add_argument("-o", help="output file name prefix without extentions", type=str, required=True)
    parser_multim.add_argument("-d", help="maximum absolute approximation error", type=int, required=True)
    parser_multim.add_argument("-l", help="list of minimizer lengths to recursively try to solve collisions.", type=int, required=True, nargs='+')
    parser_multim.add_argument("-c", help="csf file to append to", type=str, required=False)
    parser_multim.add_argument("-w", help="working directory for the temporary files. If not specified all temporary files will be deleted", type=str, required=False)
'''

'''Exploratory analysis things
def collapse_main(args):
    if (args.m == "min"): collapse = min
    elif (args.m == "avg"): collapse = lambda l: sum(l)/len(l)
    elif (args.m == "max"): collapse = max
    elif (args.m == "set"): collapse = set
    elif (args.m == "maj"): collapse = majority
    with open(args.o, "w") as fo:
        with open(args.i, "r") as fd:
            for line in fd:
                mm, *freqs = line.split()
                freqs = list(map(int, freqs))
                if (not args.d or (max(freqs) - min(freqs)) < args.d): collapsed = collapse(freqs)
                else: collapsed = args.r
                fo.write("{} {}\n".format(mm, collapsed))

def group_main(args):
    if (args.i): th = open(args.i, "r")
    else: th = sys.stdin
    buckets = group_func(th, args.m, args.p, args.d, args.r)
    th.close()
    if (args.o): fo = open(args.o, "w")
    else: fo = sys.stdout
    for mm, collapsed in buckets.items():
        fo.write("{} {}\n".format(mm, collapsed))
    fo.close()

def filter_main(args):
    buckets = dict()
    m = None
    with open(args.b, "r") as bh:
        for line in bh:
            mm, val = line.split()
            if (m and m != len(mm)): raise ValueError("minimizers with different lengths")
            m = len(mm)
            #val = float(val) if is_number(val) else val
            buckets[mm] = smart_numeric_cast(val)
    if (args.i): fd = open(args.i, "r")
    else: fd = sys.stdin
    if (args.o): fo = open(args.o, "w")
    else: fo = sys.stdout
    for line in fd:
        kmer, count = line.split()
        mm = get_mm(m, kmer)
        if(args.s and buckets[mm] == str(args.r)):#if val == label then pass
            fo.write("{} {}\n".format(kmer, count))
        elif(not args.s and buckets[mm] != str(args.r)):#inverted logic: if val != label then pass
            fo.write("{} {}\n".format(kmer, count))
    fo.close()
    fd.close()

def diff_main(args):
    buckets = dict()
    m = None
    with open(args.b, "r") as bh:
        for line in bh:
            mm, val = line.split()
            if (m and m != len(mm)): raise ValueError("minimizers with different lengths")
            m = len(mm)
            #val = float(val) if is_number(val) else val
            buckets[mm] = smart_numeric_cast(val)
    if (args.i): fd = open(args.i, "r")
    else: fd = sys.stdin
    if (args.o): fo = open(args.o, "w")
    else: fo = sys.stdout
    for line in fd:
        kmer, count = line.split()
        mm = get_mm(m, kmer)
        fo.write("{} {}\n".format(kmer, abs(int(count) - buckets[mm])))
    fo.close()
    fd.close()

def simple_check_main(args):
    buckets = dict()
    m = None
    with open(args.b, "r") as bh:
        for line in bh:
            mm, val = line.split()
            if (m and m != len(mm)): raise ValueError("minimizers with different lengths")
            m = len(mm)
            #val = float(val) if is_number(val) else val
            buckets[mm] = smart_numeric_cast(val)
    if (args.i): fd = open(args.i, "r")
    else: fd = sys.stdin
    total_errors = 0
    sod = 0
    dmax = 0
    L1 = 0
    for line in fd:
        L1 += 1
        kmer, count = line.split()
        mm = get_mm(m, kmer)
        if(is_number(buckets[mm])):
            count = int(count)
            delta = abs(count - buckets[mm])
            sod += delta
            if (delta > dmax): dmax = delta
            if (delta > 1e-10): total_errors += 1
    fd.close()
    print("Sum of errors = ", sod)
    print("Number of errors = {} (L1 = {})".format(total_errors, L1))
    print("Average error = ", sod/total_errors)
    print("Maximum error = ", dmax)
'''

'''Bloom related stuff
def bloom_filter_dimensioning(histogram: list, epsilon: float):
    histogram.sort(reverse=True)
    light_items = sum(histogram[1:])
    R = bloom_hashes(epsilon)
    bl1_dim = bloom_size(light_items, epsilon)
    expected_fp_number = histogram[0] * epsilon
    bl2_dim = bloom_size(expected_fp_number, epsilon)
    return R, bl1_dim, bl2_dim

def dim_main(args):
    columns = list()
    if (args.i): hh = open(args.i, "r")
    else: hh = sys.stdin
    for line in hh:
        _, col = line.split()
        col = int(col)
        columns.append(col)
    hh.close()
    R, bl1, bl2 = bloom_filter_dimensioning(columns, args.e)
    
    #print("Number of cold items = {}".format(n))
    print("Number of hash functions = {}".format(R))
    print("Size of the first bloom filter = {} bits = {} bytes".format(bl1, math.ceil(bl1/8)))
    #print("Number of expected false positives = {}".format(math.ceil(fps1)))
    print("Size of the second bloom filter = {} bits = {} bytes".format(bl2, math.ceil(bl2/8)))
    print("Total space = {} bits = {} bytes".format(bl1 + bl2, math.ceil((bl1 + bl2)/8)))

def bloom_main(args):
    histo = dict()
    cbkts = tempfile.NamedTemporaryFile()
    with open(args.i, "r") as th:
        buckets = group_func(th, args.m, "maj", None, None)
        ucbkts = tempfile.NamedTemporaryFile()
        with open(ucbkts.name, "w") as tmp:
            for mm, collapsed in buckets.items():
                tmp.write("{} {}\n".format(mm, collapsed))
        run_GOV3(ucbkts.name, cbkts.name)
        th.seek(0)
        for line in th:
            kmer, count = line.split()
            mm = get_mm(args.m, kmer)
            diff = abs(int(count) - buckets[mm])
            if diff in histo: histo[diff] += 1
            else: histo[diff] = 1
    columns = list(histo.values())
    _, bl1, bl2 = bloom_filter_dimensioning(columns, args.e)
    csf_size = os.stat(cbkts.name).st_size
    bloom_size = math.ceil((bl1 + bl2) / 8)
    print("(bloom filter size) + (csf size) = {} + {} = {}".format(bloom_size, csf_size, bloom_size + csf_size))

def bloom2_main(args):
    compressed_buckets_file = tempfile.NamedTemporaryFile()
    with open(args.i, "r") as th:
        buckets = group_func(th, args.m, "maj", None, None)#filling buckets by majority vote, no threshold, no replacement
    
    bucket_histo = dict()
    dummy = tempfile.NamedTemporaryFile()
    with open(dummy.name, "w") as tmp:
        for mm, collapsed in buckets.items():
            tmp.write("{} {}\n".format(mm, collapsed))
            if collapsed in bucket_histo: bucket_histo[collapsed] += 1
            else: bucket_histo[collapsed] = 1
    run_GOV3(dummy.name, compressed_buckets_file.name)
    #H0 = compute_entropy(sorted(list(bucket_histo.values()), reverse=True))
    #sys.stderr.write("H0 of bucket array of length {} = {}. Expected memory usage: {} bytes\n".format(len(buckets), H0, round(H0 * len(buckets)) / 8))
    
    with open(args.i, "r") as th:
        histo = dict()
        for line in th:
            kmer, count = line.split()
            mm = get_mm(args.m, kmer)
            diff = abs(int(count) - buckets[mm])
            if diff in histo: histo[diff] += 1
            else: histo[diff] = 1

    columns = list(histo.values())
    R, bl1, bl2 = bloom_filter_dimensioning(columns, args.e)
    hfuncs = []
    bloom1 = [False for _ in range(bl1)]
    bloom2 = [False for _ in range(bl2)]
    random.seed(args.seed)
    for _ in range(R):
        hfuncs.append(pyhash.murmur3_x64_128(random.randint(0, 2**31-1)))

    with open(args.i, "r") as th:#populate first filter
        for line in th:
            kmer, count = line.split()
            mm = get_mm(args.m, kmer)
            diff = abs(int(count) - buckets[mm])
            if diff:
                for h in hfuncs:
                    bloom1[h(kmer) % bl1] = True #insert k-mers with non-zero differences

    with open(args.i, "r") as th:#populate second filter
        for line in th:
            kmer, count = line.split()
            mm = get_mm(args.m, kmer)
            diff = abs(int(count) - buckets[mm])
            if diff == 0:
                query = True
                for h in hfuncs: query = query and bloom1[h(kmer) % bl1]
                if query:#false positive found
                    for h in hfuncs:
                        bloom2[h(kmer) % bl2] = True #insert false positive into the second bloom filter
    
    table1 = tempfile.NamedTemporaryFile()
    table2 = tempfile.NamedTemporaryFile()
    tb1h = open(table1.name, "w")
    tb2h = open(table2.name, "w")
    with open(args.i, "r") as th:#filter false positives for zero and non-zero differences
        for line in th:
            kmer, count = line.split()
            mm = get_mm(args.m, kmer)
            diff = abs(int(count) - buckets[mm])
            query1 = query2 = True
            for h in hfuncs: 
                query1 = query1 and bloom1[h(kmer) % bl1]
                query2 = query2 and bloom2[h(kmer) % bl2]
            if query1:#diff != 0 ?
                if query2:#possible false positive with diff == 0
                    if diff != 0:#explicitly save into the fallback structure
                        tb2h.write("{} {}\n".format(kmer, diff))
                    else:#it is a true false positive, do not save it
                        pass
                else:#yes, diff != 0
                    tb1h.write("{} {}\n".format(kmer, diff))
    tb1h.close()
    tb2h.close()
    compressed_diff_not_0 = tempfile.NamedTemporaryFile()
    run_GOV3(table1.name, compressed_diff_not_0.name)

    buckets_size = os.stat(compressed_buckets_file.name).st_size
    blooms_size = math.ceil((bl1 + bl2) / 8)
    csf_size = os.stat(compressed_diff_not_0.name).st_size
    fallback_size = os.stat(table2.name).st_size
    total = buckets_size + blooms_size + csf_size + fallback_size

    with open(args.i, "r") as th:
        kmer, count = th.readline().split()
        k = len(kmer)
    with open(args.o, "a") as appout:
        appout.write("{},{},{},{},{},{},{}\n".format(k, args.m, buckets_size, blooms_size, csf_size, fallback_size, total))
'''

'''Multiple minimizer related functions
def read_mm_buckets(bh):
    mm, _ = bh.readline().split()
    m = len(mm)
    bh.seek(0)
    buckets = dict()
    for line in bh:
        mm, val = line.split()
        if (m != len(mm)): raise ValueError("minimizers with different lengths")
        #val = float(val) if is_number(val) else None
        buckets[mm] = smart_numeric_cast(val)
    return m, buckets

def build_main(args):
    """Build a recursive minimizer count tables.

    --Algorithm description--
    input: 
        count table T, 
        maximum difference D, 
        a list L of different minimizer lengths [m0, m1, ..., k] with m0 < m1 < ... < k
            NOTE: the last minimizer length is the k-mer length of the counting table.

    For each mm length m in L:
        1. build table T_m of (minimizer, frequency) by bucketing the k-mers of T.
        2. if a bucket containg frequencies with difference > D, mark it as colliding (frequency = None)
           else collapse all frequencies into one (by minimum)
        3. extract all colliding records from table T to create a sub-table Y. NOTE that Y stores (k-mers, frequency) pairs as T
        4. save the buckets of m-mers
        5. T = Y
    |L| bucketing tables will be generated, one for each different minimizer length.
    Each table can then be compressed by the most suitable mean (CSF, MPHF, set-min).
    The maximum error when retrieving a frequency will be D by construction.

    An additional CSF mapping k-mers to errors can be used to answer exact queries.
    """
    args.l.sort()
    with open(args.i, "r") as fi:
        kmer, _ = fi.readline().split()
        k = len(kmer)
        if (k < args.l[-1]): raise ValueError("minimizer length greater than k-mer length")
        if (k > args.l[-1]): args.l.append(k)
    temp_out = args.i
    fname = os.path.basename(args.i).split('.')[0]
    bucket_tables = []
    for q in args.l:
        with open(temp_out, "r") as ft:
            buckets = dict()
            for line in ft:#bucketing + min collapse
                kmer, count = line.split()
                mm = get_mm(q, kmer)
                count = int(count)
                k = len(kmer)
                if mm in buckets:
                    val = buckets[mm]
                    if (val != None):
                        val[0] = min(val[0], count)
                        val[1] = max(val[1], count)
                        if(val[1] - val[0] > args.d): buckets[mm] = None
                        else: buckets[mm] = val
                else: buckets[mm] = [count, count]
            ft.seek(0)
            if (not args.w and temp_out != args.i): os.remove(temp_out)
            temp_out = fname + '.t{}'.format(q) + '.txt'
            if (args.w): temp_out = os.path.join(args.w, temp_out)
            with open(temp_out, "w") as fo:#filtering
                for line in ft:
                    kmer, count = line.split()
                    mm = get_mm(q, kmer)
                    if(buckets[mm] == None): fo.write("{} {}\n".format(kmer, count))
            with open(args.o + ".m{}".format(q) + ".txt", "w") as bo:#save bucket table
                for mm, collapsed in buckets.items():
                    if(collapsed): bo.write("{} {}\n".format(mm, collapsed[0]))
                    else: bo.write("{} {}\n".format(mm, 0))#collapsed))
            bucket_tables.append(buckets)
    if (not args.w and temp_out != args.i): os.remove(temp_out)#remove last tmp file
    with open(args.o + ".delta.txt", "w") as dt:
        with open(args.i, "r") as ft:#delta table creation
            for line in ft:
                kmer, count = line.split()
                count = int(count)
                i = freq = 0
                while(not freq):
                    mm = get_mm(args.l[i], kmer)
                    freq = bucket_tables[i][mm]
                    i += 1
                #sys.stderr.write("{}\n".format(count))
                #sys.stderr.write("{}\n".format(freq))
                dt.write("{} {}\n".format(kmer, abs(count - freq[0])))

def bcompress_main(args):
    """Compress all bucket tables using GOV3 CSF"""
    for q in args.l:
        fin = args.i + ".m{}".format(q) + ".txt"
        fout = args.o + ".m{}".format(q) + ".csf"
        run_GOV3(fin, fout)
    run_GOV3(args.i+".delta.txt", args.o+".delta.csf")

def build_check_main(args):
    #load mm bucket tables
    buckets = dict()
    if (args.i): fd = open(args.i, "r")
    else: fd = sys.stdin
    bkt_names = args.buckets
    for bkt_name in bkt_names:
        with open(bkt_name, "r") as bd:
            m, bucket = read_mm_buckets(bd)
            buckets[m] = bucket
    sys.stderr.write("mm lengths = {}\n".format(list(buckets.keys())))
    mm_lens = sorted(list(buckets.keys()))
    total_errors = sod = dmax = L1 = 0
    for line in fd:
        L1 += 1
        kmer, count = line.split()
        val = None
        mmidx = 0
        while(not val):
            mm = get_mm(mm_lens[mmidx], kmer)
            val = buckets[mm_lens[mmidx]][mm]
            mmidx += 1
        count = int(count)
        delta = abs(count - val)
        sod += delta
        if (delta > dmax): dmax = delta
        if (delta > 0): total_errors += 1 
    fd.close()
    print("Sum of errors = ", sod)
    print("Number of errors = {} (L1 = {})".format(total_errors, L1))
    print("Average error = ", sod/total_errors)
    print("Maximum error = ", dmax)
'''

'''Final versions of Bloom+CSF, mm+Bloom, multiple before their unification
def group_func(table_handle, minimizer_length: int, collapse_policy: str, delta: int, replacement: str) -> dict:
    def majority(bucket: list):
        histo = dict()
        for elem in bucket:
            if elem in histo: histo[elem] += 1
            else: histo[elem] = 1
        return max(histo.items(), key=operator.itemgetter(1))[0]
    buckets = dict()
    for line in table_handle:
        km, count = line.split()
        mm = kmer.minimizer(minimizer_length, km, hasher)
        count = int(count)
        if mm in buckets: buckets[mm].append(count)
        else: buckets[mm] = [count]
    
    if (collapse_policy == "min"): collapse = min
    elif (collapse_policy == "avg"): collapse = lambda l: sum(l)/len(l)
    elif (collapse_policy == "max"): collapse = max
    elif (collapse_policy == "set"): collapse = set
    elif (collapse_policy == "maj"): collapse = majority
    elif (collapse_policy == "id"): collapse = lambda l: l
    else: raise ValueError("collapsing method not recognised")
    for mm, freqs in buckets.items():
        if (not delta or (max(freqs) - min(freqs)) < delta): buckets[mm] = collapse(freqs)
        else: buckets[mm] = args.r
    return buckets

def bloom_main(args):
    gen = random.Random()
    gen.seed(args.seed)
    sp = kmer.Spectrum()
    sp.addFromFile(args.i)
    heavy_element = sp.getMaxCount()
    if (args.e == None): args.e = sp.getOptimalEpsilon(sp._L1light)
    blf = bloom.BloomFilter(sp._L1light, args.e, gen, pyhash.murmur3_x64_128)
    tmp_name = get_random_name("txt")
    with open(tmp_name, "w") as tmp:
        collisions = 0
        with open(args.i, "r") as th:#populating bloom filter and filtering
            k = kmer.length(th)
            for line in th:
                km, count = line.split()
                count = smart_numeric_cast(count)
                if count != heavy_element:#add light items to bloom filter
                    tmp.write("{} {}\n".format(km, count))
                    blf.add(km)
        with open(args.i, "r") as th:#false positives
            for line in th:
                km, count = line.split()
                count = smart_numeric_cast(count)
                if count == heavy_element and (km in blf):#check heavy items only for collisions
                    tmp.write("{} {}\n".format(km, count))
                    collisions += 1#save it in the collision table
    cmp_name = get_random_name("bin")
    run_GOV3(tmp_name, cmp_name)
    csf_size = os.stat(cmp_name).st_size
    os.remove(cmp_name)
    os.remove(tmp_name)
    with open(args.o, "a") as of:
        of.write("{},{},{},{},{},{}\n".format(k, args.e, blf.getBytesMemory(), csf_size, blf.getBytesMemory() + csf_size, collisions))

def get_difference(counter, representative):
    diff = counter - representative
    if diff < 0: return diff + 2**32
    return diff

def blfmm_main(args):
    gen = random.Random()
    gen.seed(args.seed)
    sp = kmer.Spectrum()
    sp.addFromFile(args.i)
    with open(args.i, "r") as th: buckets = group_func(th, args.m, "maj", None, None)#filling buckets by majority vote, no threshold, no replacement

    buckets_file = get_random_name("txt")
    with open(buckets_file, "w") as bktsh:
        for mm, collapsed in buckets.items():
            bktsh.write("{} {}\n".format(mm, collapsed))

    #begin new bloom filter for buckets
    bloom_output = get_random_name("csv")
    class MyArgs:
        def __init__(self):
            self.i = self.o = self.e = self.seed = None
    myargs = MyArgs()
    myargs.i = buckets_file
    myargs.o = bloom_output
    myargs.seed = args.seed
    bloom_main(myargs)
    with open(bloom_output, "r") as blod: content = blod.readline()
    os.remove(bloom_output)
    bloom_output = content.strip().split(',')
    buckets_size = int(bloom_output[4])
    #end

    #compressed_buckets_file = get_random_name("bin")
    #run_GOV3(buckets_file, compressed_buckets_file)
    #buckets_size = os.stat(compressed_buckets_file).st_size
    #os.remove(compressed_buckets_file)
    os.remove(buckets_file)
    
    with open(args.i, "r") as th:#compute histogram of the deltas
        k = kmer.length(th)
        delta_histo = dict()
        for line in th:
            km, count = line.split()
            mm = kmer.minimizer(args.m, km, hasher)
            #diff = abs(int(count) - buckets[mm])
            diff = get_difference(int(count), buckets[mm])
            if diff in delta_histo: delta_histo[diff] += 1
            else: delta_histo[diff] = 1
    mdelta = max(delta_histo.values())
    L1_light = 0
    with open(args.i, "r") as th:#compute L1 norm of light deltas
        for line in th:
            km, count = line.split()
            mm = kmer.minimizer(args.m, km, hasher)
            #diff = abs(int(count) - buckets[mm])
            diff = get_difference(int(count), buckets[mm])
            if delta_histo[diff] != mdelta:
                L1_light += smart_numeric_cast(count)#the L1 norm depends on the k-mer counts, not the deltas!
    if (args.e == None): args.e = sp.getOptimalEpsilon(L1_light)
    blf = bloom.BloomFilter(L1_light, args.e, gen, pyhash.murmur3_x64_128)
    #sys.stderr.write("L1 norm: {}\nepsilon: {}\nL1 norm of light items: {}\nbloom filter size: {}\n".format(sp.L1Norm(), args.e, L1_light, blf.size))
    with open(args.i, "r") as th:#populate filter
        for line in th:
            km, count = line.split()
            mm = kmer.minimizer(args.m, km, hasher)
            #diff = abs(int(count) - buckets[mm])
            diff = get_difference(int(count), buckets[mm])
            if diff != 0:
                blf.add(km)

    tmp_name = get_random_name("txt")
    with open(tmp_name, "w") as tmp:
        with open(args.i, "r") as th:#check false positives + non-zero elements
            for line in th:
                km, count = line.split()
                mm = kmer.minimizer(args.m, km, hasher)
                #diff = abs(int(count) - buckets[mm])
                diff = get_difference(int(count), buckets[mm])
                if diff == 0:
                    if km in blf:
                        tmp.write("{} {}\n".format(km, diff))
                else:
                    tmp.write("{} {}\n".format(km, diff))
    csf_name = get_random_name("bin")
    run_GOV3(tmp_name, csf_name)
    csf_size = os.stat(csf_name).st_size
    os.remove(tmp_name)
    os.remove(csf_name)
    with open(args.o, "a") as appout:
        appout.write("{},{},{},{},{},{},{}\n".format(k, args.m, args.e, buckets_size, blf.getBytesMemory(), csf_size, buckets_size+blf.getBytesMemory()+csf_size))

def multiple_main(args):
    args.l.sort()
    with open(args.i, "r") as fi:
        k = kmer.length(fi)
        if (k < args.l[-1]): raise ValueError("minimizer length greater than k-mer length")
        if (k > args.l[-1]): args.l.append(k)
    temp_out = args.i
    fname = os.path.basename(args.i).split('.')[0]
    bucket_tables_csize = 0
    bucket_tables = []
    for q in args.l:
        with open(temp_out, "r") as ft:
            buckets = dict()
            for line in ft:#bucketing + min collapse
                km, count = line.split()
                mm = kmer.minimizer(q, km, hasher)
                count = int(count)
                if mm in buckets:
                    val = buckets[mm]
                    if (val != None):
                        val[0] = min(val[0], count)
                        val[1] = max(val[1], count)
                        if(val[1] - val[0] > args.d): buckets[mm] = None
                        else: buckets[mm] = val
                else: buckets[mm] = [count, count]
            ft.seek(0)
            if (not args.w and temp_out != args.i): os.remove(temp_out)
            temp_out = fname + '.t{}'.format(q) + '.txt'
            if (args.w): temp_out = os.path.join(args.w, temp_out)
            with open(temp_out, "w") as fo:#filtering
                for line in ft:
                    km, count = line.split()
                    mm = kmer.minimizer(q, km, hasher)
                    if(buckets[mm] == None): fo.write("{} {}\n".format(km, count))
            out_file = args.o + ".m{}".format(q) + ".txt"
            with open(out_file, "w") as bo:#save bucket table
                for mm, collapsed in buckets.items():
                    if(collapsed): bo.write("{} {}\n".format(mm, collapsed[0]))
                    else: bo.write("{} {}\n".format(mm, 0))#collapsed))
            bucket_tables.append(buckets)
            if(args.c):
                csf_tmp = get_random_name("bin")
                run_GOV3(out_file, csf_tmp)
                bucket_tables_csize += os.stat(csf_tmp).st_size
                os.remove(csf_tmp)
    if (not args.w and temp_out != args.i): os.remove(temp_out)#remove last tmp file
    delta_file = args.o + ".delta.txt"
    with open(delta_file, "w") as dt:
        with open(args.i, "r") as ft:#delta table creation
            for line in ft:
                km, count = line.split()
                count = int(count)
                i = freq = 0
                while(not freq):
                    mm = kmer.minimizer(args.l[i], km, hasher)
                    freq = bucket_tables[i][mm]
                    i += 1
                #sys.stderr.write("{}\n".format(count))
                #sys.stderr.write("{}\n".format(freq))
                dt.write("{} {}\n".format(km, abs(count - freq[0])))
    if(args.c):
        run_GOV3(delta_file, csf_tmp)
        delta_size = os.stat(csf_tmp).st_size
        os.remove(csf_tmp)
        with open(args.c, "a") as appfile:
            appfile.write("{},{},{},{},{}\n".format(k, '-'.join(map(str, args.l)), args.d, bucket_tables_csize, delta_size))
'''
